{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FB-Decipherer/Experiment-17/blob/main/April_24_Word_Clusters.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 304,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19xmdPrLm9_n",
        "outputId": "3ea772b6-03fe-4a0d-ed0f-8c0e36fef7ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 305,
      "metadata": {
        "id": "rLZe7ao_4_zV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f62c9b6-edf2-439a-a46f-1954261de6eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from google.colab import files\n",
        "\n",
        "import operator\n",
        "from datetime import date\n",
        "\n",
        "import pandas as pd\n",
        "import csv\n",
        "import re\n",
        "\n",
        "from IPython.display import HTML\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk import sent_tokenize\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter('ignore', FutureWarning)\n",
        "\n",
        "#!pip install weasyprint\n",
        "import weasyprint\n",
        "\n",
        "VERSION_DATE = '04-22-2024'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ">[create_word_dictionaries](#scrollTo=aZnkrBe57pVi)\n",
        "\n",
        ">[create_guide_word_dictionaries](#scrollTo=S0OqJetx8cyw)\n",
        "\n",
        ">[create_master_guide_word_dictionary](#scrollTo=VrdK8wJH9Guz)\n",
        "\n",
        ">[create_styled_html_pages](#scrollTo=hyCS89xC7PQk)\n",
        "\n",
        ">[create_styled_html_sentence_pages](#scrollTo=3tLxy0cK5OkR)\n",
        "\n",
        ">[create_word_clouds](#scrollTo=nY-bkOQX8tCr)\n",
        "\n"
      ],
      "metadata": {
        "colab_type": "toc",
        "id": "9qtJ6M0Rarb9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 306,
      "metadata": {
        "id": "aB-XiSLwtarw"
      },
      "outputs": [],
      "source": [
        "# Set Colab Content Root:\n",
        "\n",
        "def get_content_root():\n",
        "  return '/content/drive/MyDrive/Word Cipher/Word Clusters/'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 307,
      "metadata": {
        "id": "Q3q0_OuSutL5"
      },
      "outputs": [],
      "source": [
        "# Get Common folder Root:\n",
        "\n",
        "def get_common_root():\n",
        "  return get_content_root() + 'Common/'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 308,
      "metadata": {
        "id": "JcU3aRQnHUc6"
      },
      "outputs": [],
      "source": [
        "def get_works_root():\n",
        "  return get_content_root() + 'Works/'\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Not the Working Directory, but for one of the 16 'Works':\n",
        "\n",
        "def get_work_dir(work_name):\n",
        "\n",
        "  path = get_today_path(work_name)\n",
        "\n",
        "  if os.path.isdir(path):\n",
        "    pass\n",
        "  else:\n",
        "    print('The file ' + path + ' does not exist')\n",
        "\n",
        "  return path\n",
        "\n",
        "#print(get_work_dir('Hamlet'))"
      ],
      "metadata": {
        "id": "Akmpw0rhiA3W"
      },
      "execution_count": 309,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 310,
      "metadata": {
        "id": "5hSZJGrojmZ5"
      },
      "outputs": [],
      "source": [
        "# Get all Metadata for the Works:\n",
        "\n",
        "def get_works_metatdata():\n",
        "\n",
        "  meta_path = get_common_root() + \"All Works Metadata.csv\"\n",
        "\n",
        "  return meta_path\n",
        "\n",
        "#print(get_works_metatdata())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get path to original unaltered source text for one of the Works:\n",
        "\n",
        "def get_work_source_text(work_name):\n",
        "\n",
        "  path = get_common_root() + 'Works Source' + '/' + work_name + '.txt'\n",
        "\n",
        "  if os.path.isfile(path):\n",
        "    pass\n",
        "  else:\n",
        "    print('The file ' + path + ' does not exist')\n",
        "\n",
        "  return path\n",
        "\n",
        "#print(get_work_source_text('Hamlet'))"
      ],
      "metadata": {
        "id": "k0FnY_mLVyN0"
      },
      "execution_count": 311,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 312,
      "metadata": {
        "id": "9XB7J4qSIb6L"
      },
      "outputs": [],
      "source": [
        "# Each new day-indexed Version, create a new empty folder for each of the Works:\n",
        "\n",
        "def get_today_path(work_name):\n",
        "\n",
        "  today_path = get_content_root() + get_creation_day() + '/' + work_name + '/'\n",
        "\n",
        "  if not os.path.isdir(today_path):\n",
        "    os.makedirs(today_path, exist_ok=True) # go ahead and overwrite everything.\n",
        "\n",
        "  return today_path\n",
        "\n",
        "#get_today_path('Othello')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 313,
      "metadata": {
        "id": "3ehxiKRbIjmo"
      },
      "outputs": [],
      "source": [
        "# Folder name for all the Works is over form 04-14-2024:\n",
        "\n",
        "def get_creation_day():\n",
        "\n",
        "  creation_day = date.today().strftime(\"04-22-2024\")\n",
        "\n",
        "  return creation_day\n",
        "\n",
        "#print(get_creation_day())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 314,
      "metadata": {
        "id": "nuYl6tKFeGqC"
      },
      "outputs": [],
      "source": [
        "# Get the Path Names of the Source Texts as a list, read from a .csv file:\n",
        "\n",
        "def get_work_names():\n",
        "\n",
        "  source_file_metadata = get_works_metatdata()\n",
        "\n",
        "  with open(source_file_metadata, 'r') as csvfile:\n",
        "\n",
        "      # Create a CSV reader object\n",
        "      reader = csv.reader(csvfile)\n",
        "\n",
        "      # Read the csv header row\n",
        "      header = next(reader)\n",
        "\n",
        "      # Create an empty list to store the strings from the last column\n",
        "      source_file_names = []\n",
        "\n",
        "      # Iterate over the rows in the CSV file\n",
        "      for row in reader:\n",
        "          # Get the string from the last column\n",
        "          work_name = row[-1]\n",
        "          source_file_names.append(work_name)\n",
        "\n",
        "  csvfile.close()\n",
        "\n",
        "  return source_file_names\n",
        "\n",
        "#print(get_work_names())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 315,
      "metadata": {
        "id": "Jdb9VJt_2Q4r"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_work_words_dict(work_name):\n",
        "\n",
        "  word_count_file_path = get_content_root() + get_creation_day() + '/' + work_name + '/' + work_name + '.csv'\n",
        "\n",
        "  with open(word_count_file_path, \"r\") as f:\n",
        "    word_count_dict = f.read()\n",
        "\n",
        "    print(work_name)\n",
        "    #print(word_count_dict)\n",
        "\n",
        "    reader = csv.DictReader(f)\n",
        "    data = {}\n",
        "    for row in reader:\n",
        "      data[row[\"Word\"]] = row\n",
        "\n",
        "  return data\n",
        "\n",
        "#print(get_work_words_dict('Othello'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 316,
      "metadata": {
        "id": "USVyaEsKYRC9"
      },
      "outputs": [],
      "source": [
        "# Create new Works directories for each Work for Today if not existing:\n",
        "\n",
        "today_file_path = get_content_root() + get_creation_day() + '/'\n",
        "\n",
        "os.makedirs(today_file_path, exist_ok = True)\n",
        "\n",
        "work_name_list = get_work_names()\n",
        "\n",
        "for work_name in work_name_list:\n",
        "\n",
        "  work_dir = today_file_path + work_name + '/'\n",
        "  os.makedirs(work_dir, exist_ok = True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 317,
      "metadata": {
        "id": "dq-b-bAGxDjL"
      },
      "outputs": [],
      "source": [
        "# Make Source Text All Lower case:\n",
        "\n",
        "def remove_uppercase(text):\n",
        "  return text.lower()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 318,
      "metadata": {
        "id": "-mxYOnic5cZ9"
      },
      "outputs": [],
      "source": [
        "# Remove Source Text Punctuation:\n",
        "\n",
        "def remove_punctuation(text):\n",
        "\n",
        "  new_text = \"\"\n",
        "  return re.sub(r'[^\\w\\s]', '', text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 319,
      "metadata": {
        "id": "DO8TO_f6Vvvx"
      },
      "outputs": [],
      "source": [
        "# Limit Size of Dictionary Values:\n",
        "\n",
        "def limit_dict_values(dict1, value):\n",
        "  \"\"\"Removes all items from a dictionary if the value is less than the input.\n",
        "\n",
        "  Args:\n",
        "    dict: The dictionary to remove the item from.\n",
        "    value: The value limit for removal (int).\n",
        "\n",
        "  Returns:\n",
        "    A new dictionary with the items removed.\n",
        "  \"\"\"\n",
        "\n",
        "  dict2 = {}\n",
        "  for key, val in dict1.items():\n",
        "    if val >= value:\n",
        "      dict2[key] = val\n",
        "\n",
        "  return dict2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 320,
      "metadata": {
        "id": "tYVNm85d5RX1"
      },
      "outputs": [],
      "source": [
        "# Create a Dictionary of the Word Occurrances in One Source Text:\n",
        "\n",
        "def create_word_count_dict(filepath, word_occurrance_min, word_length_min):\n",
        "  \"\"\"\n",
        "\n",
        "  Args:\n",
        "    filename: The path to the text file\n",
        "    word_occurrance_min: Min value for inclusion in the dict\n",
        "    word_length_min: Min value for inclusion in the dict\n",
        "\n",
        "  Returns:\n",
        "    A dict of words with word counts\n",
        "  \"\"\"\n",
        "\n",
        "  f = open(filepath, \"r\")\n",
        "\n",
        "  # Read the text file into a string.\n",
        "  text = f.read()\n",
        "\n",
        "  text = remove_punctuation(text)\n",
        "  text = remove_uppercase(text)\n",
        "\n",
        "  # Split the text into a list of words.\n",
        "  words = text.split()\n",
        "\n",
        "  # Create a dictionary to store the word counts.\n",
        "  dict_word_counts = {}\n",
        "\n",
        "  for word in words:\n",
        "    if word not in dict_word_counts:\n",
        "      dict_word_counts[word] = 0\n",
        "\n",
        "    if(len(word) > word_length_min):\n",
        "      dict_word_counts[word] += 1\n",
        "\n",
        "  # Remove items from the dictionary if the value is less than specified:\n",
        "  dict_copy = limit_dict_values(dict_word_counts, word_occurrance_min)\n",
        "\n",
        "  return dict_copy\n",
        "\n",
        "#print(create_word_count_dict(\"/content/drive/MyDrive/Word Cipher/Word Clusters/Common/Works Source/A Midsummer Night's Dream.txt\", 5, 5))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display a dataframe for a dict in a csv file:\n",
        "\n",
        "def display_csv_dict_in_dataframe(csv_dict_path):\n",
        "\n",
        "  # Read the CSV file into a dictionary\n",
        "  with open(csv_dict_path, 'r') as f:\n",
        "\n",
        "    reader = csv.DictReader(f)\n",
        "    dict_data = list(reader)\n",
        "\n",
        "    # Convert the dictionary to a Pandas DataFrame\n",
        "    df = pd.DataFrame(dict_data)\n",
        "\n",
        "    return df\n",
        "\n",
        "#display_csv_dict_in_dataframe(\"/content/drive/MyDrive/Word Cipher/Word Clusters/Common/All Works Metadata.csv\")"
      ],
      "metadata": {
        "id": "9sOernKevRgB"
      },
      "execution_count": 321,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display a dict in dataframe:\n",
        "\n",
        "def display_word_dict_in_dataframe(word_dict):\n",
        "\n",
        "  # Read the CSV file into a dictionary\n",
        "  with open(word_dict, 'r') as f:\n",
        "\n",
        "    reader = csv.DictReader(f)\n",
        "    dict_data = list(reader)\n",
        "\n",
        "    # Convert the dictionary to a Pandas DataFrame\n",
        "    df = pd.DataFrame(dict_data)\n",
        "\n",
        "    return df\n",
        "\n",
        "#csv_dict_path = \"/content/drive/MyDrive/Word Cipher/Word Clusters/04-14-2024/All's Well That Ends Well/All's Well That Ends Well guide words.csv\"\n",
        "#with open(csv_dict_path, 'r') as f:\n",
        "##  reader = csv.DictReader(f)\n",
        "#  dict_data = list(reader)\n",
        "  #print(display_word_dict_in_dataframe(dict_data))\n",
        "#  display(display_word_dict_in_dataframe(dict_data))"
      ],
      "metadata": {
        "id": "NJMwYiYK1SOB"
      },
      "execution_count": 322,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 323,
      "metadata": {
        "id": "mpxYdssqf5lo"
      },
      "outputs": [],
      "source": [
        "# Limit Size of Dictionary Count Values:\n",
        "\n",
        "def sort_descending(dict1):\n",
        "  \"\"\"Creates and returns a new dictionary with the one column sorted by value, descending.\n",
        "\n",
        "  Args:\n",
        "    dict1: The dictionary to sort.\n",
        "\n",
        "  Returns:\n",
        "    A new dictionary with the columns sorted by value.\n",
        "  \"\"\"\n",
        "\n",
        "  dict2 = {}\n",
        "\n",
        "  for key, value in sorted(dict1.items(), key=operator.itemgetter(1), reverse=True):\n",
        "    dict2[key] = value\n",
        "\n",
        "  return dict2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 324,
      "metadata": {
        "id": "6ju_pHwb6Gag"
      },
      "outputs": [],
      "source": [
        "\n",
        "def save_word_occurrences( work_name, word_occurrences_dict):\n",
        "\n",
        "  word_occurrences_dict = sort_descending(word_occurrences_dict)\n",
        "  word_count_file_path = get_work_dir(work_name) + work_name + '.csv'\n",
        "\n",
        "  # Open the new CSV file in write mode\n",
        "  with open(word_count_file_path, 'w', newline='') as csvfile:\n",
        "      # Create a CSV writer object\n",
        "      writer = csv.writer(csvfile)\n",
        "\n",
        "      # Write the header row\n",
        "      writer.writerow(['Word', 'occurrence'])\n",
        "\n",
        "      # Write the dictionary to the CSV file\n",
        "      for key, value in word_occurrences_dict.items():\n",
        "          writer.writerow([key, value])\n",
        "\n",
        "  # Close the CSV file\n",
        "  csvfile.close()\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# create_word_dictionaries"
      ],
      "metadata": {
        "id": "aZnkrBe57pVi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 325,
      "metadata": {
        "id": "VxtSoLT-5ELv"
      },
      "outputs": [],
      "source": [
        "# Create and Save a Filtered Dictionary of Word Occurrances for Each Source Text:\n",
        "\n",
        "def create_word_dictionaries():\n",
        "\n",
        "  work_name_list = get_work_names()\n",
        "\n",
        "  word_count_min  = 5\n",
        "  word_length_min = 5\n",
        "\n",
        "  for work_name in work_name_list:\n",
        "\n",
        "    source_file_path = get_work_source_text(work_name)\n",
        "\n",
        "    dict_word_counts = create_word_count_dict(source_file_path,\n",
        "                                              word_count_min, word_length_min)\n",
        "\n",
        "    #print(dict_word_counts)\n",
        "\n",
        "    # dict_word_counts = remove_external_stop_words(dict_word_counts)\n",
        "    # dict_word_counts = remove_internal_stop_words(dict_word_counts)\n",
        "    # dict_word_counts = remove_work_specfic_stop_words(work_name, dict_word_counts)\n",
        "\n",
        "    #display_word_dict_in_dataframe(dict_word_counts)\n",
        "\n",
        "    save_word_occurrences(work_name, dict_word_counts)\n",
        "\n",
        "#create_word_dictionaries()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gqhle-2S8ZuJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# create_guide_word_dictionaries"
      ],
      "metadata": {
        "id": "S0OqJetx8cyw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 326,
      "metadata": {
        "id": "reRLJtdIF8_q"
      },
      "outputs": [],
      "source": [
        "# Create and Save a Dictionary of Guide Word Occurrances for Each Source Text:\n",
        "\n",
        "def create_guide_word_dictionaries():\n",
        "\n",
        "  work_name_list = get_work_names()\n",
        "\n",
        "  for work_name in work_name_list:\n",
        "\n",
        "    work_guide_words_dict = get_work_guide_words(work_name)\n",
        "\n",
        "    save_guide_word_occurrences(work_name, work_guide_words_dict)\n",
        "\n",
        "#create_guide_word_dictionaries()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 327,
      "metadata": {
        "id": "StuyZy94Ho5z"
      },
      "outputs": [],
      "source": [
        "# Get List of Guide Words:\n",
        "\n",
        "guide_word_list = [\"fortune\", \"nature\", \"honour\", \"reputation\"]\n",
        "\n",
        "def get_guide_word_list():\n",
        "  return guide_word_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 328,
      "metadata": {
        "id": "4TP_0h9FIrAZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_work_guide_words(work_name):\n",
        "\n",
        "  gwl = get_guide_word_list()\n",
        "  gwsl = get_guide_word_substring_list()\n",
        "\n",
        "  work_guide_words = {}\n",
        "\n",
        "  source_file_path = get_work_source_text(work_name)\n",
        "\n",
        "  with open(source_file_path, \"r\") as f:\n",
        "    file_contents = f.read()\n",
        "    file_contents = file_contents.lower()\n",
        "\n",
        "    gwsl = get_guide_word_substring_list()\n",
        "\n",
        "    for string in gwsl:\n",
        "      occurrences = re.findall(string, file_contents)\n",
        "      full_str = get_full_str(string)\n",
        "      work_guide_words[full_str] = len(occurrences)\n",
        "\n",
        "  return work_guide_words\n",
        "\n",
        "#print(get_work_guide_words('Hamlet'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 329,
      "metadata": {
        "id": "b7v4gbP-wrj-"
      },
      "outputs": [],
      "source": [
        "##save_guide_word_occurrences\n",
        "\n",
        "def save_guide_word_occurrences( work_name, word_occurrences_dict):\n",
        "\n",
        "  word_occurrences_dict = sort_descending(word_occurrences_dict)\n",
        "\n",
        "  #df = pd.DataFrame(columns=['Word', 'Occurrances'])\n",
        "\n",
        "  guide_word_count_file_path = get_work_dir(work_name) + work_name  + ' guide words' + '.csv'\n",
        "\n",
        "  #print(guide_word_count_file_path)\n",
        "\n",
        " # Open a CSV file in write mode\n",
        "  with open(guide_word_count_file_path, \"w\", newline=\"\") as csvfile:\n",
        "\n",
        "    # Create a CSV writer object\n",
        "    writer = csv.writer(csvfile)\n",
        "\n",
        "    # Write the header row\n",
        "    writer.writerow([\"Count\", \"Occurrences\"])\n",
        "\n",
        "    # Write the data rows\n",
        "    for row in word_occurrences_dict.items():\n",
        "        writer.writerow(row)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# create_master_guide_word_dictionary"
      ],
      "metadata": {
        "id": "VrdK8wJH9Guz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 330,
      "metadata": {
        "id": "PcFQjQoj_Bvu"
      },
      "outputs": [],
      "source": [
        "# Get List of Substring Guide Words:\n",
        "\n",
        "guide_word_substring_list = [\"fortun\", \"natur\", \"hono\", \"reput\"]\n",
        "\n",
        "def get_guide_word_substring_list():\n",
        "  return guide_word_substring_list\n",
        "\n",
        "\n",
        "def get_guide_word_substring(whole_word):\n",
        "\n",
        "  dict = {}\n",
        "  for i in range(len(guide_word_list)):\n",
        "    dict[guide_word_list[i]] = guide_word_substring_list[i]\n",
        "\n",
        "  return dict[whole_word]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 331,
      "metadata": {
        "id": "myBhoAXoqCPw"
      },
      "outputs": [],
      "source": [
        "# Remove Dictionary Items:\n",
        "\n",
        "def remove_dict_entries(dict1, list1):\n",
        "  \"\"\"Removes entries from a dict based on a list of keys.\n",
        "\n",
        "  Args:\n",
        "    dict1: The dict to remove entries from.\n",
        "    list1: The list of keys to remove.\n",
        "\n",
        "  Returns:\n",
        "    An updated dict with the entries removed.\n",
        "  \"\"\"\n",
        "\n",
        "  dict2 = {}\n",
        "  for key, value in dict1.items():\n",
        "    if key not in list1:\n",
        "      dict2[key] = value\n",
        "\n",
        "  return dict2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 332,
      "metadata": {
        "id": "zbxEPgHU1mbY"
      },
      "outputs": [],
      "source": [
        "# Remove External Stop Words:\n",
        "\n",
        "def remove_external_stop_words(ext_stop_words_dict):\n",
        "\n",
        "  ext_stop_words = nltk.corpus.stopwords.words('english')\n",
        "  ext_stop_words_dict = dict.fromkeys(ext_stop_words)\n",
        "\n",
        "  # Show the external stop words:\n",
        "  df_4 = pd.DataFrame.from_dict(ext_stop_words_dict, orient=\"index\", columns=[\"The Ext Stop Words\"])\n",
        "  # df_4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 333,
      "metadata": {
        "id": "JAbTWkYqWpdW"
      },
      "outputs": [],
      "source": [
        "# Remove Internal Stop Words:\n",
        "\n",
        "def remove_internal_stop_words(source_text):\n",
        "\n",
        "  my_stop_words = [\"things\", \"within\" , \"should\", \"before\",\"though\",\"againe\",\"live\", \"bring\", \"finde\", \"heare\", \"see\", \"leaue\" , \"ere\", \"put\", \"giue\", \"shee\", \"hee\",\"haue\", \"heere\", \"come\", \"make\", \"let\",\"take\", \"give\", \"heere\", \"par\", \"shall\", \"lord\"]\n",
        "\n",
        "  dict_word_counts = remove_dict_entries(dict_word_counts, my_stop_words)\n",
        "\n",
        "  df_3 = pd.DataFrame.from_dict(dict_word_counts, orient=\"index\", columns=[\"Minus My Stop Words\"])\n",
        "  #df_3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 334,
      "metadata": {
        "id": "VXOj1Ulez9Vb"
      },
      "outputs": [],
      "source": [
        "# Remove Work-specific Stop Words:\n",
        "\n",
        "def remove_work_specfic_stop_words(work_name, dict_word_counts):\n",
        "\n",
        "  source_file_path = get_work_dir(work_name) + work_name + \".csv\"\n",
        "\n",
        "  if os.path.isfile(source_file_path):\n",
        "    with open(source_file_path, \"r\") as f:\n",
        "      reader = csv.reader(f)\n",
        "      first_row = next(reader)\n",
        "      # Save the first column of the CSV file as a list\n",
        "      first_column_list = [row[0] for row in reader]\n",
        "      dict_word_counts = remove_dict_entries(dict_word_counts, first_column_list)\n",
        "  else:\n",
        "    #print('The file ' + source_file_path + ' does not exist')\n",
        "    pass\n",
        "\n",
        "  return dict_word_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 335,
      "metadata": {
        "id": "fg7WvGVQFK3I"
      },
      "outputs": [],
      "source": [
        "#888\n",
        "def get_guide_word_count_for_work(work_name, guide_word):\n",
        "\n",
        "  # Open the CSV file\n",
        "\n",
        "  word_count_file_path =  get_work_dir(work_name) + work_name + ' guide words' + '.csv'\n",
        "\n",
        "  # print(word_count_file_path)\n",
        "\n",
        "\n",
        "  with open(word_count_file_path, 'r') as f:\n",
        "\n",
        "      # Create a CSV reader object\n",
        "      reader = csv.reader(f)\n",
        "\n",
        "      # Read the header row\n",
        "      header = next(reader)\n",
        "\n",
        "      # Create a dictionary to store the data\n",
        "      data = {}\n",
        "\n",
        "      # Iterate over the rows in the CSV file\n",
        "      for row in reader:\n",
        "          # Get the key and value from the row\n",
        "          key, value = row[0], row[1]\n",
        "\n",
        "          # Add the key-value pair to the dictionary\n",
        "          data[key] = value\n",
        "\n",
        "  # Look up the value for the given key\n",
        "  key = guide_word\n",
        "  gwc = data.get(key)\n",
        "\n",
        "  return gwc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 336,
      "metadata": {
        "id": "MsAMV76MWPBt"
      },
      "outputs": [],
      "source": [
        "# Get HTML for Color Coding Words:\n",
        "\n",
        "def get_guide_word_html(guide_word, color):\n",
        "\n",
        "  style = {\"color\": color, \"text-decoration\": \"none\", \"font-weight\": \"bold\"}\n",
        "  html_output = generate_html_for_inline_styles_of_text(guide_word, style)\n",
        "\n",
        "  return html_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 337,
      "metadata": {
        "id": "J2a9j_8_5Ttm"
      },
      "outputs": [],
      "source": [
        "# Get HTML for Styling Words:\n",
        "\n",
        "def generate_html_for_inline_styles_of_text(text, style):\n",
        "  \"\"\"Generates HTML for inline style of text.\n",
        "\n",
        "  Args:\n",
        "    text: The text to be styled.\n",
        "    style: A dictionary of CSS properties and values.\n",
        "\n",
        "  Returns:\n",
        "    A string containing the HTML for the inline style of text.\n",
        "  \"\"\"\n",
        "\n",
        "  html = \"<span style='{}'>{}</span>\".format(\n",
        "      \"; \".join([\"{}: {}\".format(k, v) for k, v in style.items()]), text)\n",
        "\n",
        "  return html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 338,
      "metadata": {
        "id": "J6DOxWZTm6ei"
      },
      "outputs": [],
      "source": [
        "def list_to_html_table(list_of_strings):\n",
        "  \"\"\"Converts a list of strings into an HTML table.\n",
        "\n",
        "  Args:\n",
        "    list_of_strings: A list of strings.\n",
        "\n",
        "  Returns:\n",
        "    An HTML table string.\n",
        "  \"\"\"\n",
        "\n",
        "  for sentence_string in list_of_strings:\n",
        "    guide_word = 'fortune'\n",
        "    color = 'red'\n",
        "\n",
        "  df = pd.DataFrame(list_of_strings, columns=[\"Unfiltered Word Counts\"])\n",
        "\n",
        "  html_table = \"\"\"<table border=1 >\"\"\"\n",
        "  for string in list_of_strings:\n",
        "    html_table += \"\"\"<tr><td>{}</td></tr>\"\"\".format(string)\n",
        "  html_table += \"\"\"</table>\"\"\"\n",
        "\n",
        "  return html_table\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkisw_WWw1r4"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 339,
      "metadata": {
        "id": "c9RRZYZ1rcEl"
      },
      "outputs": [],
      "source": [
        "def replace_substring(list_of_str, substring, new_string):\n",
        "  \"\"\"Replaces each item in a list of str with\n",
        "  #  a substr in each item replaced by the same str.\n",
        "\n",
        "  Args:\n",
        "    list_of_str: A list of str.\n",
        "    substring: The substring to be replaced.\n",
        "    new_string: The new string to replace the substring with.\n",
        "\n",
        "  Returns:\n",
        "    A list of str with the substring replaced in each item.\n",
        "  \"\"\"\n",
        "\n",
        "  new_list_of_str = []\n",
        "  for item in list_of_str:\n",
        "    new_item = item.replace(substring, new_string)\n",
        "    new_list_of_str.append(new_item)\n",
        "  return new_list_of_str\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 340,
      "metadata": {
        "id": "Wk-t4E53hk0P"
      },
      "outputs": [],
      "source": [
        "def get_guide_sentences(source_file_path, which_guide_word):\n",
        "\n",
        "  with open(source_file_path, \"r\") as f:\n",
        "    file_contents = f.read()\n",
        "    sentences = file_contents.split(\".\")\n",
        "\n",
        "    guide_sentences = []\n",
        "    for sentence in sentences:\n",
        "      if \"fortune\" in sentence:\n",
        "          guide_sentences.append(sentence)\n",
        "\n",
        "  return guide_sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 341,
      "metadata": {
        "id": "jHQ2dAf186OY"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_guide_word_color(guide_word):\n",
        "\n",
        "\tguide_word_colors = {\n",
        "\t    \"fortune\": \"red\",\n",
        "\t    \"nature\": \"green\",\n",
        "\t    \"honour\": \"blue\",\n",
        "\t    \"reputation\": \"orange\"\n",
        "\t}\n",
        "\n",
        "\treturn guide_word_colors[guide_word]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 342,
      "metadata": {
        "id": "1E923IpPX850"
      },
      "outputs": [],
      "source": [
        "def dict_to_html_table(dict):\n",
        "  \"\"\"Converts a dictionary to an HTML table.\n",
        "\n",
        "  Args:\n",
        "    dict: A dictionary.\n",
        "\n",
        "  Returns:\n",
        "    An HTML table string.\n",
        "  \"\"\"\n",
        "\n",
        "  html = \"<table>\"\n",
        "  for key, value in dict.items():\n",
        "    html += \"<tr><td>{}</td><td>{}</td></tr>\".format(key, value)\n",
        "  html += \"</table>\"\n",
        "  return html\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PxXlH7Z77OGG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 343,
      "metadata": {
        "id": "bGb_gh7DraXP"
      },
      "outputs": [],
      "source": [
        "def get_full_str(sub_str):\n",
        "\n",
        "    match sub_str:\n",
        "        case \"fortun\":\n",
        "            return \"fortune\"\n",
        "        case \"natur\":\n",
        "            return \"nature\"\n",
        "        case \"hono\":\n",
        "            return \"honour\"\n",
        "        case \"reput\":\n",
        "            return \"reputation\"\n",
        "        case _:\n",
        "            return \"Invalid string\"\n",
        "\n",
        "    return full_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 344,
      "metadata": {
        "id": "aW2uQUvnjZjD"
      },
      "outputs": [],
      "source": [
        "# Create and Save a Dictionary of Guide Word Occurrances in Common dir:\n",
        "\n",
        "def create_master_guide_word_dictionary():\n",
        "\n",
        "  fortune_counts = []\n",
        "  nature_counts = []\n",
        "  honour_counts = []\n",
        "  reputation_counts = []\n",
        "\n",
        "  df = pd.DataFrame()\n",
        "\n",
        "  work_name_list = get_work_names()\n",
        "\n",
        "  for work_name in work_name_list:\n",
        "\n",
        "      work_guide_words_dict = get_work_guide_words(work_name)\n",
        "\n",
        "      fortune_counts.append(work_guide_words_dict['fortune'])\n",
        "      nature_counts.append(work_guide_words_dict['nature'])\n",
        "      honour_counts.append(work_guide_words_dict['honour'])\n",
        "      reputation_counts.append(work_guide_words_dict['reputation'])\n",
        "\n",
        "      #df = pd.DataFrame.from_dict(work_guide_words_dict, orient=\"index\", columns=[\"\"])\n",
        "      #print(df)\n",
        "\n",
        "  df = pd.DataFrame({'Work': work_name_list})\n",
        "  df = df.assign(Fortune=fortune_counts)\n",
        "  df = df.assign(Nature=nature_counts)\n",
        "  df = df.assign(Honour=honour_counts)\n",
        "  df = df.assign(Reputation=reputation_counts)\n",
        "\n",
        "  #print(df)\n",
        "  #display(df)\n",
        "\n",
        "  path = get_common_root() + \"Master Guide Word Occurrances.csv\"\n",
        "\n",
        "  df.to_csv(path, index=False)\n",
        "\n",
        "#print(create_master_guide_word_dictionary())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# create_styled_html_pages"
      ],
      "metadata": {
        "id": "hyCS89xC7PQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#For each Work, color code the Guide Words and save as a html file:\n",
        "\n",
        "def create_styled_html_pages():\n",
        "\n",
        "  source_file_metadata = get_works_metatdata()\n",
        "  work_name_list = get_work_names()\n",
        "  guide_word_list = get_guide_word_list()\n",
        "\n",
        "  for work_name in work_name_list:\n",
        "\n",
        "    source_file_path = get_work_source_text(work_name)\n",
        "\n",
        "    # Open the source text file of the Work:\n",
        "    with open(source_file_path, \"r\") as f:\n",
        "      file_contents = f.read()\n",
        "\n",
        "    file_contents = remove_uppercase(file_contents)\n",
        "\n",
        "    for guide_word in guide_word_list:\n",
        "\n",
        "      # Set the color coding for the full source text page:\n",
        "      color = get_guide_word_color(guide_word)\n",
        "      guide_word_html = get_guide_word_html(guide_word, color)\n",
        "\n",
        "      # Splice in HTML for a colored Span tag:\n",
        "      file_contents = file_contents.replace(guide_word, guide_word_html)\n",
        "\n",
        "    # end for guide_word in guide_word_list\n",
        "\n",
        "    words_output_file_path = get_today_path(work_name) + 'Guide Words Page Text' + '.html'\n",
        "\n",
        "    message = \"<b><br><br>To Do:</b> <br><br> save pages as PDF, then subsample down to a PNG with size suitable for Machine Vision, then use existing Clustering 'maths' to determing maximum guideword clusteirng<br><br>\"\n",
        "    # Open new output file in write mode, and write the final page HTML:\n",
        "    with open(words_output_file_path, \"w\") as output_html_file:\n",
        "      file_contents = message +  file_contents\n",
        "      output_html_file.write(file_contents)\n",
        "\n",
        "    # end 'for each guide word'\n",
        "  # end for Words, each Work"
      ],
      "metadata": {
        "id": "m2E-iue6PsJk"
      },
      "execution_count": 345,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 346,
      "metadata": {
        "id": "4cbpb0cigd_m"
      },
      "outputs": [],
      "source": [
        "def appy_styling_to_html(totals_html_table):\n",
        "\n",
        "  guide_word_list = get_guide_word_list()\n",
        "\n",
        "  for guide_word in guide_word_list:\n",
        "    color = get_guide_word_color(guide_word)\n",
        "    guide_word_html = get_guide_word_html(guide_word, color)\n",
        "\n",
        "    totals_html_table = re.sub(guide_word, guide_word_html, totals_html_table)\n",
        "\n",
        "  return totals_html_table"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# create_styled_html_sentence_pages"
      ],
      "metadata": {
        "id": "3tLxy0cK5OkR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 347,
      "metadata": {
        "id": "zsPzdE4G_v9v"
      },
      "outputs": [],
      "source": [
        "# For each Work, split text into sentences and\n",
        "# color code the Guide Words, and save as a html file:\n",
        "\n",
        "def create_styled_html_sentence_pages():\n",
        "\n",
        "  guide_sentence_count_dict = {}\n",
        "  styled_guide_sentences = []\n",
        "\n",
        "  work_name_list = get_work_names()\n",
        "\n",
        "  for work_name in work_name_list:\n",
        "\n",
        "    source_file_path = get_work_source_text(work_name)\n",
        "\n",
        "    # Open the source text file of the Work:\n",
        "    with open(source_file_path, \"r\") as f:\n",
        "      file_contents = f.read()\n",
        "\n",
        "    file_contents = remove_uppercase(file_contents)\n",
        "\n",
        "    # Split file text into sentences:\n",
        "    sentence_list = file_contents.split(\".\")\n",
        "\n",
        "    # add color coding to all sentences with guide words in them:\n",
        "    for sentence in sentence_list:\n",
        "      word_list = sentence.split()\n",
        "\n",
        "      for i in range(len(word_list)):\n",
        "        if word_list[i] in guide_word_list:\n",
        "\n",
        "          color = get_guide_word_color(word_list[i])\n",
        "          guide_word_html = get_guide_word_html(word_list[i], color)\n",
        "          word_list[i] = word_list[i].replace(word_list[i], guide_word_html)\n",
        "\n",
        "          sentence_string = \" \".join(word_list)\n",
        "          styled_guide_sentences.append(sentence_string)\n",
        "\n",
        "    # Create a HTML table of color coded sentences:\n",
        "    page_html_guide_sentences = list_to_html_table(styled_guide_sentences)\n",
        "\n",
        "    # For Header, open template file in Common dir for substitutions:\n",
        "    page_html_path = get_common_root() + 'guide_sentences_template.html'\n",
        "\n",
        "    with open(page_html_path, \"r\") as f:\n",
        "          html_template_file_contents = f.read()\n",
        "\n",
        "    # Make the substitutions into a copy of the Header template:\n",
        "    html_template_file_contents = html_template_file_contents.replace(\"Page_Title\", work_name)\n",
        "    html_template_file_contents = html_template_file_contents.replace(\"Page_Heading\", work_name)\n",
        "\n",
        "    # Get occurrences of guide words:\n",
        "    fortune_count = get_guide_word_count_for_work(work_name, 'fortune')\n",
        "    nature_count = get_guide_word_count_for_work(work_name, 'nature')\n",
        "    honour_count = get_guide_word_count_for_work(work_name, 'honour')\n",
        "    reputation_count = get_guide_word_count_for_work(work_name, 'reputation')\n",
        "\n",
        "    # Get overall total of guide words for the Work:\n",
        "    work_total_guide_words = int(fortune_count) + int(nature_count) + int(honour_count) + int(reputation_count)\n",
        "\n",
        "    word_occurrance_dict = {\"fortune\": fortune_count, \"nature\": nature_count, \"honour\": honour_count, \"reputation\": reputation_count }\n",
        "\n",
        "    # Create color-coded HTML table of guide word occurrences for Header:\n",
        "    totals_html_table = dict_to_html_table(word_occurrance_dict)\n",
        "    styled_totals_html_table = appy_styling_to_html(totals_html_table)\n",
        "    html_template_file_contents += styled_totals_html_table + '<br/>'\n",
        "\n",
        "    # Finish making file header:\n",
        "    html_template_file_contents = html_template_file_contents.replace(\"Guide_Sentences\",\n",
        "          '<b>' + str(work_total_guide_words) + '</b>' + ' Sentences with Guide Words' + '</b>' + '<br/>')\n",
        "\n",
        "    # Add HTML header to HTML sentences body:\n",
        "    page_html = html_template_file_contents + page_html_guide_sentences\n",
        "\n",
        "    # Open new output file in write mode, and write the final HTML:\n",
        "    html_output_file_path = get_work_dir(work_name) + work_name + ' Guide Words Sentences' + '.html'\n",
        "\n",
        "    with open(html_output_file_path, \"w\") as output_html_file:\n",
        "      output_html_file.write(page_html)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 348,
      "metadata": {
        "id": "S0KxIagQmKfI"
      },
      "outputs": [],
      "source": [
        "def read_text_file(file_path):\n",
        "  \"\"\"Reads a text file and returns a list of words.\"\"\"\n",
        "  with open(file_path, \"r\") as f:\n",
        "    text = f.read()\n",
        "    text = text.lower()\n",
        "  words = text.split()\n",
        "  return words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 349,
      "metadata": {
        "id": "7D1l1E82qaM4"
      },
      "outputs": [],
      "source": [
        "def filter_words(words, substring):\n",
        "  \"\"\"Filters a list of words to only include words that contain a substring.\"\"\"\n",
        "  filtered_words = []\n",
        "  for word in words:\n",
        "    if substring in word:\n",
        "      filtered_words.append(word)\n",
        "  return filtered_words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 350,
      "metadata": {
        "id": "Z38eAk2YreHr"
      },
      "outputs": [],
      "source": [
        "def read_text_file(file_path):\n",
        "  \"\"\"Reads a text file and returns a list of words.\"\"\"\n",
        "  with open(file_path, \"r\") as f:\n",
        "    text = f.read()\n",
        "    text = text.lower()\n",
        "  words = text.split()\n",
        "  return words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 351,
      "metadata": {
        "id": "YouFE_8FHteY"
      },
      "outputs": [],
      "source": [
        "def get_guide_word_dict(work_name):\n",
        "\n",
        "  dict_file_path = get_work_dir(work_name) + work_name + ' guide words' + '.csv'\n",
        "\n",
        "  with open(dict_file_path, 'r') as f:\n",
        "    reader = csv.DictReader(f)\n",
        "\n",
        "    data_dict = {}\n",
        "    for row in reader:\n",
        "        data_dict[row['Count']] = row\n",
        "\n",
        "  return data_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 352,
      "metadata": {
        "id": "Fq2Q60wdXLyj"
      },
      "outputs": [],
      "source": [
        "def display_dataframe(guide_word, guide_word_counts, display_row_count):\n",
        "\n",
        "  df = pd.DataFrame.from_dict(guide_word_counts, orient=\"index\")\n",
        "\n",
        "  column_name = \"Occurrances of '\" + guide_word + \"'\"\n",
        "  df.set_axis([column_name], axis=1, inplace=True)\n",
        "\n",
        "  #df = df.append({column_name: df[column_namdf1 = df.set_index('project')\n",
        "  #df1 = df.set_index('project')\n",
        "  df.loc['Total'] = df.sum(numeric_only=True)\n",
        "\n",
        "  display(df.head(display_row_count))\n",
        "\n",
        "  print(\"\\n\")\n",
        "\n",
        "  return\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import csv\n",
        "\n",
        "def create_word_cloud(work_name):\n",
        "\n",
        "  word_occurrences = {}\n",
        "\n",
        "  word_count_file_path =  get_work_dir(work_name) + work_name + '.csv'\n",
        "  with open(word_count_file_path, 'r') as f:\n",
        "    reader = csv.DictReader(f)\n",
        "\n",
        "    for row in reader:\n",
        "        word_occurrences[row['Word']] = int(row['occurrence'])\n",
        "\n",
        "    # Create a WordCloud object\n",
        "    wordcloud = WordCloud()\n",
        "\n",
        "    # Generate the word cloud\n",
        "    wordcloud.generate_from_frequencies(word_occurrences)\n",
        "\n",
        "    # Display the word cloud\n",
        "    #plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "    #plt.axis(\"off\")\n",
        "    #plt.show()\n",
        "\n",
        "  word_count_save_path =  get_work_dir(work_name) + work_name + '.png'\n",
        "  wordcloud.to_file(word_count_save_path)\n",
        "\n",
        "#print(create_word_cloud('Othello'))"
      ],
      "metadata": {
        "id": "NwAjqnn09-KS"
      },
      "execution_count": 353,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# create_word_clouds"
      ],
      "metadata": {
        "id": "nY-bkOQX8tCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_word_clouds():\n",
        "\n",
        "  work_name_list = get_work_names()\n",
        "\n",
        "  for work_name in work_name_list:\n",
        "    create_word_cloud(work_name)\n"
      ],
      "metadata": {
        "id": "_PH3y93SLXQL"
      },
      "execution_count": 354,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_styled_pdf_pages():\n",
        "\n",
        "  work_name_list = get_work_names()\n",
        "\n",
        "  for work_name in work_name_list:\n",
        "\n",
        "    html_path = open(get_work_dir(work_name) + \"Guide Words Page Text\" + '.html', 'r', encoding='utf-8')\n",
        "    #print(html_path)\n",
        "    source_code = html_path.read()\n",
        "\n",
        "    html = HTML(string=source_code)\n",
        "    pdf = html.write_pdf()\n",
        "\n",
        "    with open(get_work_dir(work_name) + \"Guide Words Page Text\" + '.pdf', 'wb') as f:\n",
        "        f.write(pdf)\n",
        "\n",
        "#print(create_styled_pdf_pages())"
      ],
      "metadata": {
        "id": "7aCHAjtEcckD"
      },
      "execution_count": 372,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# create_styled_pdf_pages"
      ],
      "metadata": {
        "id": "I_Yq2uEPAXHN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 373,
      "metadata": {
        "id": "QHoWI4gOFBVG"
      },
      "outputs": [],
      "source": [
        "def word_clusters():\n",
        "\n",
        "  create_word_dictionaries()\n",
        "\n",
        "  create_guide_word_dictionaries()\n",
        "\n",
        "  create_master_guide_word_dictionary()\n",
        "\n",
        "  #create_word_clouds()\n",
        "\n",
        "  create_styled_html_pages()\n",
        "\n",
        "  create_styled_html_sentence_pages()\n",
        "\n",
        "  #create_styled_pdf_pages()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ">[create_word_dictionaries](#scrollTo=aZnkrBe57pVi)\n",
        "\n",
        ">[create_guide_word_dictionaries](#scrollTo=S0OqJetx8cyw)\n",
        "\n",
        ">[create_master_guide_word_dictionary](#scrollTo=VrdK8wJH9Guz)\n",
        "\n",
        ">[create_styled_html_pages](#scrollTo=hyCS89xC7PQk)\n",
        "\n",
        ">[create_styled_html_sentence_pages](#scrollTo=3tLxy0cK5OkR)\n",
        "\n",
        ">[create_word_clouds](#scrollTo=nY-bkOQX8tCr)\n",
        "\n",
        ">[create_styled_pdf_pages](#scrollTo=I_Yq2uEPAXHN)\n",
        "\n"
      ],
      "metadata": {
        "colab_type": "toc",
        "id": "L0BWCXq-a4Ux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_clusters()"
      ],
      "metadata": {
        "id": "-oAHSpLt12ar"
      },
      "execution_count": 357,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}